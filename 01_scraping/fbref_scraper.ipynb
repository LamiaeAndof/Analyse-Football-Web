{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d47a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dbefaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping : Real Madrid\n",
      "Scraping : Barcelona\n",
      "Scraping : Manchester City\n",
      "Scraping : Paris SG\n",
      "Scraping : Bayern Munich\n",
      "Scraping : Arsenal\n",
      "Scraping : Liverpool\n",
      "Scraping : Juventus\n",
      "Scraping : AC Milan\n",
      "Scraping : Chelsea\n"
     ]
    }
   ],
   "source": [
    "# üìå Liste √©tendue de clubs √† scraper\n",
    "clubs = {\n",
    "    \"Real Madrid\": \"https://fbref.com/en/squads/53a2f082/Real-Madrid-Stats\",\n",
    "    \"Barcelona\": \"https://fbref.com/en/squads/206d90db/Barcelona-Stats\",\n",
    "    \"Manchester City\": \"https://fbref.com/en/squads/b8fd03ef/Manchester-City-Stats\",\n",
    "    \"Paris SG\": \"https://fbref.com/en/squads/e2d8892c/Paris-Saint-Germain-Stats\",\n",
    "    \"Bayern Munich\": \"https://fbref.com/en/squads/054efa67/Bayern-Munich-Stats\",\n",
    "    \"Arsenal\": \"https://fbref.com/en/squads/18bb7c10/Arsenal-Stats\",\n",
    "    \"Liverpool\": \"https://fbref.com/en/squads/822bd0ba/Liverpool-Stats\",\n",
    "    \"Juventus\": \"https://fbref.com/en/squads/e0652b02/Juventus-Stats\",\n",
    "    \"AC Milan\": \"https://fbref.com/en/squads/dc56fe14/Milan-Stats\",\n",
    "    \"Chelsea\": \"https://fbref.com/en/squads/cff3d9bb/Chelsea-Stats\"\n",
    "}\n",
    "\n",
    "# üß† Fonction de scraping principale\n",
    "def scrape_club_stats(club_name, url):\n",
    "    print(f\"Scraping : {club_name}\")\n",
    "    tables = pd.read_html(url)\n",
    "\n",
    "    # Table 0 = General stats (toujours pr√©sente)\n",
    "    df = tables[0].copy()\n",
    "    df[\"Club\"] = club_name  # Ajouter nom club\n",
    "\n",
    "    # Ajouter d'autres tables disponibles si elles existent\n",
    "    columns = [\"Player\", \"Nation\", \"Pos\", \"Age\", \"MP\", \"Starts\", \"Min\", \"Gls\", \"Ast\", \"xG\", \"xA\", \"Tkl\", \"Int\", \"Blocks\", \"CrdY\", \"CrdR\"]\n",
    "\n",
    "    # Garder seulement les colonnes utiles si elles sont l√†\n",
    "    df = df[[col for col in columns if col in df.columns] + [\"Club\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "# üîÅ Boucle sur les clubs\n",
    "all_data = []\n",
    "\n",
    "for club, url in clubs.items():\n",
    "    try:\n",
    "        df = scrape_club_stats(club, url)\n",
    "        all_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur pour {club} : {e}\")\n",
    "\n",
    "# üß© Fusionner toutes les donn√©es\n",
    "final_df = pd.concat(all_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d33e0695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scraping Real Madrid\n",
      "‚ùå Erreur Real Madrid : HTTP Error 429: Too Many Requests\n",
      "üîç Scraping Barcelona\n",
      "‚ùå Erreur Barcelona : HTTP Error 429: Too Many Requests\n",
      "üîç Scraping Manchester City\n",
      "‚ùå Erreur Manchester City : HTTP Error 429: Too Many Requests\n",
      "üîç Scraping Paris SG\n",
      "‚ùå Erreur Paris SG : HTTP Error 429: Too Many Requests\n",
      "üîç Scraping Bayern Munich\n",
      "‚ùå Erreur Bayern Munich : HTTP Error 429: Too Many Requests\n",
      "üîç Scraping Liverpool\n",
      "‚ùå Erreur Liverpool : HTTP Error 429: Too Many Requests\n",
      "üîç Scraping Inter Milan\n",
      "‚ùå Erreur Inter Milan : HTTP Error 429: Too Many Requests\n",
      "‚ùå Aucun joueur r√©cup√©r√©.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clubs = {\n",
    "    \"Real Madrid\": \"https://fbref.com/en/squads/53a2f082/Real-Madrid-Stats\",\n",
    "    \"Barcelona\": \"https://fbref.com/en/squads/206d90db/Barcelona-Stats\",\n",
    "    \"Manchester City\": \"https://fbref.com/en/squads/b8fd03ef/Manchester-City-Stats\",\n",
    "    \"Paris SG\": \"https://fbref.com/en/squads/e2d8892c/Paris-Saint-Germain-Stats\",\n",
    "    \"Bayern Munich\": \"https://fbref.com/en/squads/054efa67/Bayern-Munich-Stats\", \n",
    "    \"Liverpool\": \"https://fbref.com/en/squads/822bd0ba/Liverpool-Stats\",\n",
    "    \"Inter Milan\": \"https://fbref.com/en/squads/fd962109/Internazionale-Stats\"\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for name, url in clubs.items():\n",
    "    try:\n",
    "        print(f\"üîç Scraping {name}\")\n",
    "        df = pd.read_html(url)[0]\n",
    "        df[\"Club\"] = name\n",
    "        all_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur {name} : {e}\")\n",
    "\n",
    "if all_data:\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df.to_csv(\"../data/raw/fbref_final_basic_stats.csv\", index=False)\n",
    "    print(\"‚úÖ Donn√©es sauvegard√©es dans fbref_final_basic_stats.csv\")\n",
    "else:\n",
    "    print(\"‚ùå Aucun joueur r√©cup√©r√©.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5323cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scraping Real Madrid\n",
      "‚ö†Ô∏è Aucune table trouv√©e pour Real Madrid\n",
      "üîç Scraping Barcelona\n",
      "‚ö†Ô∏è Aucune table trouv√©e pour Barcelona\n",
      "üîç Scraping Manchester City\n",
      "‚ö†Ô∏è Aucune table trouv√©e pour Manchester City\n",
      "üîç Scraping Paris SG\n",
      "‚ö†Ô∏è Aucune table trouv√©e pour Paris SG\n",
      "üîç Scraping Bayern Munich\n",
      "‚ö†Ô∏è Aucune table trouv√©e pour Bayern Munich\n",
      "üîç Scraping Liverpool\n",
      "‚ö†Ô∏è Aucune table trouv√©e pour Liverpool\n",
      "üîç Scraping Inter Milan\n",
      "‚ö†Ô∏è Aucune table trouv√©e pour Inter Milan\n",
      "‚ùå Aucun joueur r√©cup√©r√©.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "clubs = {\n",
    "    \"Real Madrid\": \"https://fbref.com/en/squads/53a2f082/Real-Madrid-Stats\",\n",
    "    \"Barcelona\": \"https://fbref.com/en/squads/206d90db/Barcelona-Stats\",\n",
    "    \"Manchester City\": \"https://fbref.com/en/squads/b8fd03ef/Manchester-City-Stats\",\n",
    "    \"Paris SG\": \"https://fbref.com/en/squads/e2d8892c/Paris-Saint-Germain-Stats\",\n",
    "    \"Bayern Munich\": \"https://fbref.com/en/squads/054efa67/Bayern-Munich-Stats\", \n",
    "    \"Liverpool\": \"https://fbref.com/en/squads/822bd0ba/Liverpool-Stats\",\n",
    "    \"Inter Milan\": \"https://fbref.com/en/squads/fd962109/Internazionale-Stats\"\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for name, url in clubs.items():\n",
    "    try:\n",
    "        print(f\"üîç Scraping {name}\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        table = soup.find(\"table\")\n",
    "        \n",
    "        if table:\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            df[\"Club\"] = name\n",
    "            all_data.append(df)\n",
    "            print(f\"‚úÖ {name} OK ({len(df)} lignes)\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Aucune table trouv√©e pour {name}\")\n",
    "\n",
    "        time.sleep(2)  # ‚è± Pause de 2 secondes pour √©viter le blocage\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur {name} : {e}\")\n",
    "\n",
    "# Fusionner et enregistrer\n",
    "if all_data:\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df.to_csv(\"../data/raw/fbref_final_basic_stats.csv\", index=False)\n",
    "    print(\"‚úÖ Donn√©es sauvegard√©es dans fbref_final_basic_stats.csv\")\n",
    "else:\n",
    "    print(\"‚ùå Aucun joueur r√©cup√©r√©.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fecd0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.32.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.0)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (4.9.0)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.32.0-py3-none-any.whl (9.4 MB)\n",
      "   ---------------------------------------- 0.0/9.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/9.4 MB 985.5 kB/s eta 0:00:09\n",
      "   --- ------------------------------------ 0.8/9.4 MB 1.0 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 1.0/9.4 MB 948.7 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.3/9.4 MB 1.1 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 1.6/9.4 MB 1.1 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 1.8/9.4 MB 1.1 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 1.8/9.4 MB 1.1 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 2.1/9.4 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 2.4/9.4 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 2.6/9.4 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 2.9/9.4 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 3.1/9.4 MB 1.1 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 3.4/9.4 MB 1.1 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 3.7/9.4 MB 1.1 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 3.9/9.4 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 4.2/9.4 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 4.5/9.4 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 4.7/9.4 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 5.0/9.4 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 5.2/9.4 MB 1.1 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 5.5/9.4 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 5.8/9.4 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 6.0/9.4 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 6.3/9.4 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 6.3/9.4 MB 1.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 6.6/9.4 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 6.8/9.4 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 7.1/9.4 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 7.3/9.4 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 7.6/9.4 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 7.9/9.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 8.1/9.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.4/9.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.7/9.4 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.9/9.4 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.9/9.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.4/9.4 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: wsproto, websocket-client, outcome, trio, trio-websocket, selenium\n",
      "\n",
      "  Attempting uninstall: websocket-client\n",
      "\n",
      "    Found existing installation: websocket-client 1.7.0\n",
      "\n",
      "    Uninstalling websocket-client-1.7.0:\n",
      "\n",
      "      Successfully uninstalled websocket-client-1.7.0\n",
      "\n",
      "   ------ --------------------------------- 1/6 [websocket-client]\n",
      "   ------ --------------------------------- 1/6 [websocket-client]\n",
      "   -------------------- ------------------- 3/6 [trio]\n",
      "   -------------------- ------------------- 3/6 [trio]\n",
      "   -------------------- ------------------- 3/6 [trio]\n",
      "   -------------------- ------------------- 3/6 [trio]\n",
      "   -------------------- ------------------- 3/6 [trio]\n",
      "   -------------------- ------------------- 3/6 [trio]\n",
      "   -------------------- ------------------- 3/6 [trio]\n",
      "   -------------------- ------------------- 3/6 [trio]\n",
      "   -------------------- ------------------- 3/6 [trio]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   --------------------------------- ------ 5/6 [selenium]\n",
      "   ---------------------------------------- 6/6 [selenium]\n",
      "\n",
      "Successfully installed outcome-1.3.0.post0 selenium-4.32.0 trio-0.30.0 trio-websocket-0.12.2 websocket-client-1.8.0 wsproto-1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script wsdump.exe is installed in 'C:\\Users\\pc\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from requests->webdriver-manager) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from requests->webdriver-manager) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from requests->webdriver-manager) (2024.2.2)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: webdriver-manager\n",
      "Successfully installed webdriver-manager-4.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (2.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install webdriver-manager\n",
    "!pip install pandas beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0f837f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scraping Real Madrid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12388\\3219197800.py:41: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Real Madrid OK (44 joueurs)\n",
      "üîç Scraping Barcelona\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12388\\3219197800.py:41: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Barcelona OK (47 joueurs)\n",
      "üîç Scraping Manchester City\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12388\\3219197800.py:41: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Manchester City OK (41 joueurs)\n",
      "üîç Scraping Paris SG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12388\\3219197800.py:41: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Paris SG OK (39 joueurs)\n",
      "üîç Scraping Bayern Munich\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12388\\3219197800.py:41: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bayern Munich OK (40 joueurs)\n",
      "üîç Scraping Liverpool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12388\\3219197800.py:41: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Liverpool OK (31 joueurs)\n",
      "üîç Scraping Inter Milan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12388\\3219197800.py:41: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inter Milan OK (30 joueurs)\n",
      "‚úÖ Donn√©es sauvegard√©es dans fbref_final_basic_stats.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ‚úÖ Configurer le navigateur (sans interface)\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Navigateur invisible\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# ‚úÖ Liste des clubs\n",
    "clubs = {\n",
    "    \"Real Madrid\": \"https://fbref.com/en/squads/53a2f082/Real-Madrid-Stats\",\n",
    "    \"Barcelona\": \"https://fbref.com/en/squads/206d90db/Barcelona-Stats\",\n",
    "    \"Manchester City\": \"https://fbref.com/en/squads/b8fd03ef/Manchester-City-Stats\",\n",
    "    \"Paris SG\": \"https://fbref.com/en/squads/e2d8892c/Paris-Saint-Germain-Stats\",\n",
    "    \"Bayern Munich\": \"https://fbref.com/en/squads/054efa67/Bayern-Munich-Stats\", \n",
    "    \"Liverpool\": \"https://fbref.com/en/squads/822bd0ba/Liverpool-Stats\",\n",
    "    \"Inter Milan\": \"https://fbref.com/en/squads/fd962109/Internazionale-Stats\"\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for club, url in clubs.items():\n",
    "    try:\n",
    "        print(f\"üîç Scraping {club}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # ‚è± Laisser la page se charger\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        table = soup.find(\"table\")\n",
    "\n",
    "        if table:\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            df[\"Club\"] = club\n",
    "            all_data.append(df)\n",
    "            print(f\"‚úÖ {club} OK ({len(df)} joueurs)\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Table non trouv√©e pour {club}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur {club} : {e}\")\n",
    "\n",
    "# Fermer le navigateur\n",
    "driver.quit()\n",
    "\n",
    "# Fusion et export\n",
    "if all_data:\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df.to_csv(\"../data/raw/fbref_final_basic_stats.csv\", index=False)\n",
    "    print(\"‚úÖ Donn√©es sauvegard√©es dans fbref_final_basic_stats.csv\")\n",
    "else:\n",
    "    print(\"‚ùå Aucun joueur r√©cup√©r√©.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d3aa708",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/pc/Analyse-Football-Web/data/raw/fbref_final_basic_stats.csv\", sep=';')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
